{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "planetGenerator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnu-chand/planetGenerator/blob/main/planetGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### init"
      ],
      "metadata": {
        "id": "XfirPSRgyy5Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idP1Khg0yjUO",
        "outputId": "8cd51d40-b877-4c43-977b-b62417a607d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pixUtils'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 196 (delta 22), reused 67 (delta 22), pack-reused 129\u001b[K\n",
            "Receiving objects: 100% (196/196), 11.70 MiB | 14.64 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=537cdc77ebb3e27342328271c11fc2530dec087382a3231be392f0fff485348e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wv1xlqjh/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.2\n",
            "    Uninstalling gdown-4.2.2:\n",
            "      Successfully uninstalled gdown-4.2.2\n",
            "Successfully installed gdown-4.4.0\n",
            "Cloning into 'FastGAN-pytorch'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 108 (delta 37), reused 40 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (108/108), 128.51 KiB | 6.76 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "if '' or not os.path.exists('/content/pixUtils'):\n",
        "    !git clone https://github.com/vishnu-chand/pixUtils.git;mv pixUtils deleteMe;mv deleteMe/pixUtils .;rm -rf deleteMe\n",
        "    !pip install --upgrade --no-cache-dir gdown\n",
        "    !git clone https://github.com/odegeasslbc/FastGAN-pytorch.git\n",
        "    from pixUtils import *\n",
        "\n",
        "import requests\n",
        "from pixUtils import *\n",
        "from urllib.parse import unquote\n",
        "\n",
        "\n",
        "def decodeUrl(line):\n",
        "    line = line.encode().decode('unicode-escape')\n",
        "    url = unquote(line)\n",
        "    while url != line:\n",
        "        url, line = unquote(line), url\n",
        "    name = basename(url)\n",
        "    for sym in '? :'.split():\n",
        "        name = name.split(sym)[0]\n",
        "    return f\"{dirname(url)}/{name}\"\n",
        "\n",
        "\n",
        "def googleSearch(htmlPath):\n",
        "    htmlPath = [h for h in htmlPath.split('\\n') if h.strip()][-1].strip()\n",
        "    with open(htmlPath, 'r') as book:\n",
        "        lines = book.read()\n",
        "    imUrls = list()\n",
        "    lines = lines.split('</script><div ng-non-bindable')[1]\n",
        "    for line in lines.split('https://')[1:]:\n",
        "        try:\n",
        "            line, w, h = line.split(',')[:3]\n",
        "            w, h = int(w), int(h.split(']')[0])\n",
        "        except:\n",
        "            continue\n",
        "        url = decodeUrl(line[:-1])\n",
        "        if 'encrypted-tbn0.gstatic.com' in url:\n",
        "            continue\n",
        "        imUrls.append([h, w, f\"https://{url}\"])\n",
        "    return imUrls\n",
        "\n",
        "\n",
        "def yandexSearch(htmlPath):\n",
        "    from bs4 import BeautifulSoup as bs4\n",
        "    htmlPath = [h for h in htmlPath.split('\\n') if h.strip()][-1].strip()\n",
        "    soup = bs4(open(htmlPath), 'html.parser')\n",
        "    res = list()\n",
        "    className = 'serp-item serp-item_type_search serp-item_group_search serp'\n",
        "    for ix, img in enumerate(soup.find_all(attrs={'class': lambda e: e.startswith(className) if e else False})):\n",
        "        data = json.loads(img.get('data-bem'))['serp-item']['preview'][0]\n",
        "        res.append((data['h'], data['w'], decodeUrl(data['url'])))\n",
        "    # res = list()\n",
        "    # className = 'serp-item__link'\n",
        "    # for ix, img in enumerate(soup.find_all(attrs={'class': lambda e: e.startswith(className) if e else False})):\n",
        "    #     imgUrl = img['href'].split('img_url=')[1].split('&')[0]\n",
        "    #     data = json.loads(img.get('data-bem'))['serp-item']['preview'][0]\n",
        "    #     res.append((data['h'], data['w'], decodeUrl(data['url'])))\n",
        "    #     raise NotImplemented\n",
        "    return res\n",
        "\n",
        "\n",
        "def downloadImg(filepath, imgUrl, wh, timeout, verbose):\n",
        "    imShape = None\n",
        "    try:\n",
        "        headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) '\n",
        "                                 'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36;'}\n",
        "        r = requests.get(imgUrl, headers=headers, stream=True, timeout=timeout)\n",
        "        with open(filepath, 'wb') as f_write:\n",
        "            for chunk in r.iter_content(1024 * 1024):\n",
        "                f_write.write(chunk)\n",
        "        img = cv2.imread(filepath)\n",
        "        if img is not None:\n",
        "            imShape = '_'.join([str(i) for i in img.shape])\n",
        "            if os.path.splitext(imgUrl)[1] != '.jpg':\n",
        "                dirop(filepath, rm=' ')\n",
        "                filepath = f\"{filename(filepath, returnPath=True)}.jpg\"\n",
        "                cv2.imwrite(filepath, img)\n",
        "        else:\n",
        "            dirop(filepath, rm=' ')\n",
        "            raise Exception(\"downloading failed\")\n",
        "    except Exception as exp:\n",
        "        if verbose:\n",
        "            print(f\"\"\"\n",
        "    {exp}\n",
        "    imgUrl   : {imgUrl}\n",
        "    filepath : {filepath}\n",
        "    imShape  : {imShape}\n",
        "    \"\"\")\n",
        "    return filepath, imgUrl, wh, imShape\n",
        "\n",
        "\n",
        "def crawler(inferencePool, desPath, imUrls, minSize, timeout, verbose):\n",
        "    desDir, desName = dirname(desPath), basename(desPath)\n",
        "    dones = [int(filename(f)) for f in rglob(f\"{desDir}/**/*.*\") if filename(f) != 'url2img']\n",
        "    print(\"len(imUrls)\", len(imUrls))\n",
        "    jobs = list()\n",
        "    for h, w, imgUrl in sorted(imUrls, key=lambda x: x[0] * x[1]):\n",
        "        ext = os.path.splitext(imgUrl)[1] or '.png'\n",
        "        fname = shaIt(imgUrl)\n",
        "        filepath = dirop(f'{desDir}/{desName}/{fname}{ext.lower()}')\n",
        "        if fname not in dones:\n",
        "            if w > minSize and h > minSize:\n",
        "                if verbose:\n",
        "                    print([w, h], fname, imgUrl)\n",
        "                jobs.append(inferencePool.submit(downloadImg, filepath=filepath, imgUrl=imgUrl, wh=f'{w}_{h}', timeout=timeout, verbose=verbose))\n",
        "    ok, nok = 0, 0\n",
        "    with open(f'{desDir}/url2img.log', 'a') as book:\n",
        "        for job in jobs:\n",
        "            filepath, imgUrl, wh, imShape = job.result()\n",
        "            if imShape:\n",
        "                msg = f\"{wh}, {imShape}, {imgUrl}, {filepath}\\n\"\n",
        "                book.write(msg)\n",
        "                ok += 1\n",
        "            else:\n",
        "                nok += 1\n",
        "    return ok, nok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainCode = '''\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import utils as vutils\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from models import weights_init, Discriminator, Generator\n",
        "from operation import copy_G_params, load_params, get_dir\n",
        "from operation import ImageFolder, InfiniteSamplerWrapper\n",
        "from diffaug import DiffAugment\n",
        "\n",
        "policy = 'color,translation'\n",
        "import lpips\n",
        "\n",
        "use_gpu = False\n",
        "if torch.cuda.is_available():\n",
        "    use_gpu = True\n",
        "\n",
        "percept = lpips.PerceptualLoss(model='net-lin', net='vgg', use_gpu=use_gpu)\n",
        "\n",
        "\n",
        "# torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def crop_image_by_part(image, part):\n",
        "    hw = image.shape[2] // 2\n",
        "    if part == 0:\n",
        "        return image[:, :, :hw, :hw]\n",
        "    if part == 1:\n",
        "        return image[:, :, :hw, hw:]\n",
        "    if part == 2:\n",
        "        return image[:, :, hw:, :hw]\n",
        "    if part == 3:\n",
        "        return image[:, :, hw:, hw:]\n",
        "\n",
        "\n",
        "def train_d(net, data, label=\"real\"):\n",
        "    \"\"\"Train function of discriminator\"\"\"\n",
        "    if label == \"real\":\n",
        "        part = random.randint(0, 3)\n",
        "        pred, [rec_all, rec_small, rec_part] = net(data, label, part=part)\n",
        "        err = F.relu(torch.rand_like(pred) * 0.2 + 0.8 - pred).mean() + \\\n",
        "              percept(rec_all, F.interpolate(data, rec_all.shape[2])).sum() + \\\n",
        "              percept(rec_small, F.interpolate(data, rec_small.shape[2])).sum() + \\\n",
        "              percept(rec_part, F.interpolate(crop_image_by_part(data, part), rec_part.shape[2])).sum()\n",
        "        err.backward()\n",
        "        return pred.mean().item(), rec_all, rec_small, rec_part\n",
        "    else:\n",
        "        pred = net(data, label)\n",
        "        err = F.relu(torch.rand_like(pred) * 0.2 + 0.8 + pred).mean()\n",
        "        err.backward()\n",
        "        return pred.mean().item()\n",
        "\n",
        "\n",
        "def get_dir(args):\n",
        "    import shutil, json\n",
        "    task_name = f'{args.saveDir}/{args.name}'\n",
        "    saved_model_folder = os.path.join(task_name, 'models')\n",
        "    saved_image_folder = os.path.join(task_name, 'images')\n",
        "\n",
        "    os.makedirs(saved_model_folder, exist_ok=True)\n",
        "    os.makedirs(saved_image_folder, exist_ok=True)\n",
        "\n",
        "    for f in os.listdir('./'):\n",
        "        if '.py' in f:\n",
        "            shutil.copy(f, task_name + '/' + f)\n",
        "\n",
        "    with open(os.path.join(saved_model_folder, '../args.txt'), 'w') as f:\n",
        "        json.dump(args.__dict__, f, indent=2)\n",
        "\n",
        "    return saved_model_folder, saved_image_folder\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    data_root = args.path\n",
        "    total_iterations = args.iter\n",
        "    checkpoint = args.ckpt\n",
        "    batch_size = args.batch_size\n",
        "    im_size = args.im_size\n",
        "    ndf = 64\n",
        "    ngf = 64\n",
        "    nz = 256\n",
        "    nlr = 0.0001\n",
        "    nbeta1 = 0.5\n",
        "    use_cuda = use_gpu\n",
        "    multi_gpu = True\n",
        "    dataloader_workers = 8\n",
        "    current_iteration = 0\n",
        "    save_interval = 10\n",
        "    saved_model_folder, saved_image_folder = get_dir(args)\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "    if use_cuda:\n",
        "        device = torch.device(\"cuda:0\")\n",
        "\n",
        "    transform_list = [\n",
        "        transforms.Resize((int(im_size), int(im_size))),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ]\n",
        "    trans = transforms.Compose(transform_list)\n",
        "\n",
        "    if 'lmdb' in data_root:\n",
        "        from operation import MultiResolutionDataset\n",
        "        dataset = MultiResolutionDataset(data_root, trans, 1024)\n",
        "    else:\n",
        "        dataset = ImageFolder(root=data_root, transform=trans)\n",
        "\n",
        "    dataloader = iter(DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
        "                                 sampler=InfiniteSamplerWrapper(dataset), num_workers=dataloader_workers, pin_memory=True))\n",
        "    \"\"\"\n",
        "    loader = MultiEpochsDataLoader(dataset, batch_size=batch_size, \n",
        "                               shuffle=True, num_workers=dataloader_workers, \n",
        "                               pin_memory=True)\n",
        "    dataloader = CudaDataLoader(loader, 'cuda')\n",
        "    \"\"\"\n",
        "\n",
        "    # from model_s import Generator, Discriminator\n",
        "    netG = Generator(ngf=ngf, nz=nz, im_size=im_size)\n",
        "    netG.apply(weights_init)\n",
        "\n",
        "    netD = Discriminator(ndf=ndf, im_size=im_size)\n",
        "    netD.apply(weights_init)\n",
        "\n",
        "    netG.to(device)\n",
        "    netD.to(device)\n",
        "\n",
        "    optimizerG = optim.AdamW(netG.parameters(), lr=nlr, betas=(nbeta1, 0.999))\n",
        "    optimizerD = optim.AdamW(netD.parameters(), lr=nlr, betas=(nbeta1, 0.999))\n",
        "\n",
        "    avg_param_G = copy_G_params(netG)\n",
        "\n",
        "    fixed_noise = torch.FloatTensor(8, nz).normal_(0, 1).to(device)\n",
        "\n",
        "    if checkpoint != 'None':\n",
        "        print(f\"loding weights from {checkpoint}\")\n",
        "        ckpt = torch.load(checkpoint, map_location=device)\n",
        "\n",
        "        netG = loadWeights(ckpt['g'], netG)\n",
        "        netD = loadWeights(ckpt['d'], netD)\n",
        "        avg_param_G = ckpt['g_ema']\n",
        "        optimizerG.load_state_dict(ckpt['opt_g'])\n",
        "        optimizerD.load_state_dict(ckpt['opt_d'])\n",
        "        # current_iteration = int(checkpoint.split('_')[-1].split('.')[0])\n",
        "        del ckpt\n",
        "\n",
        "    if multi_gpu:\n",
        "        netG = nn.DataParallel(netG.to(device))\n",
        "        netD = nn.DataParallel(netD.to(device))\n",
        "\n",
        "    for iteration in tqdm(range(current_iteration, total_iterations + 1)):\n",
        "        real_image = next(dataloader)\n",
        "        real_image = real_image.to(device)\n",
        "        current_batch_size = real_image.size(0)\n",
        "        noise = torch.Tensor(current_batch_size, nz).normal_(0, 1).to(device)\n",
        "\n",
        "        fake_images = netG(noise)\n",
        "\n",
        "        real_image = DiffAugment(real_image, policy=policy)\n",
        "        fake_images = [DiffAugment(fake, policy=policy) for fake in fake_images]\n",
        "\n",
        "        ## 2. train Discriminator\n",
        "        netD.zero_grad()\n",
        "\n",
        "        err_dr, rec_img_all, rec_img_small, rec_img_part = train_d(netD, real_image, label=\"real\")\n",
        "        train_d(netD, [fi.detach() for fi in fake_images], label=\"fake\")\n",
        "        optimizerD.step()\n",
        "\n",
        "        ## 3. train Generator\n",
        "        netG.zero_grad()\n",
        "        pred_g = netD(fake_images, \"fake\")\n",
        "        err_g = -pred_g.mean()\n",
        "\n",
        "        err_g.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        for p, avg_p in zip(netG.parameters(), avg_param_G):\n",
        "            avg_p.mul_(0.999).add_(0.001 * p.data)\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            print(\"GAN: loss d: %.5f    loss g: %.5f\" % (err_dr, -err_g.item()))\n",
        "\n",
        "        if iteration % (save_interval * 10) == 0:\n",
        "            backup_para = copy_G_params(netG)\n",
        "            load_params(netG, avg_param_G)\n",
        "            with torch.no_grad():\n",
        "                vutils.save_image(netG(fixed_noise)[0].add(1).mul(0.5), saved_image_folder + '/%d.jpg' % iteration, nrow=4)\n",
        "                vutils.save_image(torch.cat([\n",
        "                    F.interpolate(real_image, 128),\n",
        "                    rec_img_all, rec_img_small,\n",
        "                    rec_img_part]).add(1).mul(0.5), saved_image_folder + '/rec_%d.jpg' % iteration)\n",
        "            load_params(netG, backup_para)\n",
        "\n",
        "        if iteration % (save_interval * 50) == 0 or iteration == total_iterations:\n",
        "            backup_para = copy_G_params(netG)\n",
        "            load_params(netG, avg_param_G)\n",
        "            # torch.save({'g': netG.state_dict(), 'd': netD.state_dict()}, saved_model_folder + '/%d.pth' % iteration)\n",
        "            load_params(netG, backup_para)\n",
        "            torch.save({'g'    : netG.state_dict(),\n",
        "                        'd'    : netD.state_dict(),\n",
        "                        'g_ema': avg_param_G,\n",
        "                        'opt_g': optimizerG.state_dict(),\n",
        "                        'opt_d': optimizerD.state_dict()}, saved_model_folder + '/all_%d.pth' % iteration)\n",
        "\n",
        "\n",
        "def loadWeights(ckpt, netG):\n",
        "    class Temp(nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.module = model\n",
        "    try:\n",
        "        temp = Temp(netG)\n",
        "        temp.load_state_dict(ckpt)\n",
        "        model = temp.module\n",
        "    except:\n",
        "        netG.load_state_dict(ckpt)\n",
        "        model = netG\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description='region gan')\n",
        "\n",
        "    parser.add_argument('--path', type=str, default='../lmdbs/art_landscape_1k', help='path of resource dataset, should be a folder that has one or many sub image folders inside')\n",
        "    parser.add_argument('--cuda', type=int, default=0, help='index of gpu to use')\n",
        "    parser.add_argument('--name', type=str, default='test1', help='experiment name')\n",
        "    parser.add_argument('--iter', type=int, default=50000, help='number of iterations')\n",
        "    parser.add_argument('--start_iter', type=int, default=0, help='the iteration to start training')\n",
        "    parser.add_argument('--batch_size', type=int, default=6, help='mini batch number of images')\n",
        "    parser.add_argument('--im_size', type=int, default=1024, help='image resolution')\n",
        "    parser.add_argument('--ckpt', type=str, default='None', help='checkpoint weight path if have one')\n",
        "    parser.add_argument('--saveDir', type=str, default='None', help='checkpoint weight path if have one')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "\n",
        "    train(args)\n",
        "'''\n",
        "\n",
        "evalCode = '''\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import utils as vutils\n",
        "\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "from models import Generator\n",
        "\n",
        "\n",
        "def load_params(model, new_param):\n",
        "    for p, new_p in zip(model.parameters(), new_param):\n",
        "        p.data.copy_(new_p)\n",
        "\n",
        "\n",
        "def resize(img):\n",
        "    return F.interpolate(img, size=256)\n",
        "\n",
        "\n",
        "def batch_generate(zs, netG, batch=8):\n",
        "    g_images = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(zs) // batch):\n",
        "            g_images.append(netG(zs[i * batch:(i + 1) * batch]).cpu())\n",
        "        if len(zs) % batch > 0:\n",
        "            g_images.append(netG(zs[-(len(zs) % batch):]).cpu())\n",
        "    return torch.cat(g_images)\n",
        "\n",
        "\n",
        "def batch_save(images, folder_name):\n",
        "    if not os.path.exists(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "    for i, image in enumerate(images):\n",
        "        vutils.save_image(image.add(1).mul(0.5), folder_name + '/%d.jpg' % i)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "            description='generate images'\n",
        "    )\n",
        "    parser.add_argument('--ckpt', type=str)\n",
        "    parser.add_argument('--artifacts', type=str, default=\".\", help='path to artifacts.')\n",
        "    parser.add_argument('--cuda', type=int, default=None, help='index of gpu to use')\n",
        "    parser.add_argument('--start_iter', type=int, default=6)\n",
        "    parser.add_argument('--end_iter', type=int, default=10)\n",
        "\n",
        "    parser.add_argument('--dist', type=str, default='.')\n",
        "    parser.add_argument('--size', type=int, default=256)\n",
        "    parser.add_argument('--batch', default=16, type=int, help='batch size')\n",
        "    parser.add_argument('--n_sample', type=int, default=2000)\n",
        "    parser.add_argument('--big', action='store_true')\n",
        "    parser.add_argument('--im_size', type=int, default=1024)\n",
        "    parser.add_argument('--desDir', type=str, default='')\n",
        "    parser.add_argument('--seed', type=int, default=None)\n",
        "    parser.set_defaults(big=False)\n",
        "    args = parser.parse_args()\n",
        "    seed = args.seed or torch.random.seed()\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    noise_dim = 256\n",
        "    device = torch.device('cpu')\n",
        "    if args.cuda is not None:\n",
        "        device = torch.device('cuda:%d' % (args.cuda))\n",
        "\n",
        "    net_ig = Generator(ngf=64, nz=noise_dim, nc=3, im_size=args.im_size)  # , big=args.big )\n",
        "    net_ig.to(device)\n",
        "    if ' ':\n",
        "        epoch = 0\n",
        "    # for epoch in [10000 * i for i in range(args.start_iter, args.end_iter + 1)]:\n",
        "    #     ckpt = f\"{args.artifacts}/models/{epoch}.pth\"\n",
        "        ckpt = args.ckpt\n",
        "        checkpoint = torch.load(ckpt, map_location=lambda a, b: a)\n",
        "        # Remove prefix `module`.\n",
        "        checkpoint['g'] = {k.replace('module.', ''): v for k, v in checkpoint['g'].items()}\n",
        "        net_ig.load_state_dict(checkpoint['g'])\n",
        "        # load_params(net_ig, checkpoint['g_ema'])\n",
        "\n",
        "        # net_ig.eval()\n",
        "        print('load checkpoint success, epoch %d' % epoch)\n",
        "\n",
        "        net_ig.to(device)\n",
        "\n",
        "        del checkpoint\n",
        "\n",
        "        dist = f\"{args.desDir}/eval_{epoch}\"\n",
        "        os.makedirs(dist, exist_ok=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in tqdm(range(args.n_sample // args.batch)):\n",
        "                noise = torch.randn(args.batch, noise_dim).to(device)\n",
        "                g_imgs = net_ig(noise)[0]\n",
        "                g_imgs = F.interpolate(g_imgs, 512)\n",
        "                for j, g_img in enumerate(g_imgs):\n",
        "                    vutils.save_image(g_img.add(1).mul(0.5),\n",
        "                                      os.path.join(dist, '%d.png' % (i * args.batch + j)))  # , normalize=True, range=(-1,1))\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "ES6_GH-jy-q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### upload HTML"
      ],
      "metadata": {
        "id": "eoh0fZrKzDxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"\"\"\n",
        "Google image search for exoplanets and upload it html\n",
        "eg: https://www.google.com/search?q=exoplanets&tbm=isch right click save html and upload here\n",
        "\"\"\")\n",
        "a = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Mg1840J_zI1Z",
        "outputId": "37e07004-9081-4b53-e544-ab2c7131ba27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Google image search for exoplanets and upload it html\n",
            "eg: https://www.google.com/search?q=exoplanets&tbm=isch right click save html and upload here\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-00c6e761-7fa7-402e-9177-f95d8f97b87e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-00c6e761-7fa7-402e-9177-f95d8f97b87e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving exoplanets wallpaper - Google Search.html to exoplanets wallpaper - Google Search.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### step1: Build a spider to crawl into World Wide Web and collect images of exoplanets"
      ],
      "metadata": {
        "id": "VezvEa0g2Ih8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "inferencePool = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "def step1_crawler(inferencePool, htmlPaths, desPath):\n",
        "    for htmlPath in htmlPaths:\n",
        "        if 'google' in htmlPath.lower():\n",
        "            imUrls = googleSearch(htmlPath)\n",
        "        else:\n",
        "            imUrls = yandexSearch(htmlPath)\n",
        "        ok, nok = crawler(inferencePool, desPath, imUrls, minSize=384, timeout=10, verbose=False)\n",
        "        print(f\"{htmlPath}: ok, nok\", ok, nok)\n",
        "    return desPath\n",
        "\n",
        "\n",
        "# google image search for exoplanets and upload it html\n",
        "imPaths = step1_crawler(inferencePool, rglob('/content/*.html'), dirop('/content/dbImgs/unLabeledPlanets', rm=' '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chSXpTUYzVRD",
        "outputId": "d75eb532-fbf4-4044-c17c-2958f3bdd0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(imUrls) 95\n",
            "/content/exoplanets wallpaper - Google Search.html: ok, nok 71 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### step2: Train FastGAN to learn planets distribution and Wait till GAN attains John nash equilibrium"
      ],
      "metadata": {
        "id": "YWNKhxhb2hl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/db\n",
        "downloadDB('gdrive+https://drive.google.com/file/d/1RHTmh0dWM0Mg-S8_maKv5Up3m6wuwY08/view?usp=sharing', '/content/db')\n",
        "imPaths = '/content/dbImgs/unLabeledPlanets'\n",
        "saveDir = f'/content/db/results/{getTimeStamp()}'\n",
        "ckptPath = rglob('/content/db/**/*.pth')[0]\n",
        "Path('/content/FastGAN-pytorch/train.py').write_text(trainCode)\n",
        "!nvidia-smi\n",
        "!cd /content/FastGAN-pytorch;python train.py --path {imPaths} --ckpt {ckptPath} --im_size 512 --saveDir {saveDir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLXyzuBl0LpQ",
        "outputId": "1f6252e4-3fcd-4817-cab9-44955f290082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "____________________________________________________________________________________________________________________\n",
            "\n",
            "             cd \"/content/db\";gdown https://drive.google.com/uc?id=1RHTmh0dWM0Mg-S8_maKv5Up3m6wuwY08;\n",
            "\n",
            "____________________________________________________________________________________________________________________\n",
            "unzip dataset\n",
            "zip:  /content/db/good_art_1k_512.zip\n",
            "returnData:  /content/db\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "Setting up Perceptual loss...\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100% 528M/528M [00:04<00:00, 121MB/s]\n",
            "Loading model from: /content/FastGAN-pytorch/lpips/weights/v0.1/vgg.pth\n",
            "...[net-lin [vgg]] initialized\n",
            "...Done\n",
            "Namespace(batch_size=6, ckpt='/content/db/good_art_1k_512/models/all_50000.pth', cuda=0, im_size=512, iter=50000, name='test1', path='/content/dbImgs/unLabeledPlanets', saveDir='/content/db/results/Mar14_01_31_38406482', start_iter=0)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "loding weights from /content/db/good_art_1k_512/models/all_50000.pth\n",
            "  0% 0/50001 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "GAN: loss d: 0.30212    loss g: -1.27927\n",
            "  0% 1/50001 [00:53<737:17:15, 53.08s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 243, in <module>\n",
            "    train(args)\n",
            "  File \"train.py\", line 163, in train\n",
            "    fake_images = netG(noise)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/data_parallel.py\", line 150, in forward\n",
            "    return self.module(*inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/FastGAN-pytorch/models.py\", line 171, in forward\n",
            "    feat_512 = self.se_512( feat_32, self.feat_512(feat_256) )\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 141, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\", line 179, in forward\n",
            "    self.eps,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 2283, in batch_norm\n",
            "    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### step3: Generate New Planets"
      ],
      "metadata": {
        "id": "C2T3eUgo8he0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckptPath = max(rglob('/content/db/results/**/all_*.pth'), key=lambda x: int(filename(x).split('_')[1]))\n",
        "print(ckptPath)\n",
        "desDir = f'/content/db/infer/{getTimeStamp()}'\n",
        "Path('/content/FastGAN-pytorch/eval.py').write_text(evalCode)\n",
        "!cd /content/FastGAN-pytorch;python eval.py --n_sample 100 --ckpt {ckptPath} --im_size 512 --desDir {desDir} --seed 4212"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9L60E6u8n8a",
        "outputId": "8e9bf152-e6bd-4578-d8e3-fa742fc08c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/db/results/Mar14_01_31_38406482/test1/models/all_0.pth\n",
            "load checkpoint success, epoch 0\n",
            "100% 6/6 [01:26<00:00, 14.48s/it]\n"
          ]
        }
      ]
    }
  ]
}